{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b899f34",
   "metadata": {},
   "source": [
    "# DATA420-25S2 Assignment 1 â€” GHCN Data Analysis using Spark  \n",
    "**Notebook Framework (Scaffold)** Â· 2025-08-14\n",
    "\n",
    "> æœ¬ Notebook ä½œä¸º **å¯è¿è¡Œæ¡†æ¶** ä½¿ç”¨ï¼šæŒ‰é¢˜ç›®é¡ºåºé¢„ç½®äº†ä»£ç å•å…ƒä¸è¾“å‡ºå ä½ï¼Œå¹¶å†…ç½® *grading checklist*ï¼Œä»¥ä¾¿ä½ è¾¹å­¦ Lecture 9â€“12 è¾¹å®Œæˆä½œä¸šã€‚\n",
    "\n",
    "**ä½œè€…ï¼ˆAuthorï¼‰**ï¼šYu Xia  \n",
    "**å­¦å·ï¼ˆStudent IDï¼‰**ï¼š62380486\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36bbe0",
   "metadata": {},
   "source": [
    "## âœ… Grading Checklistï¼ˆå¯¹æ ‡è¯„åˆ†è¦ç‚¹ï¼‰\n",
    "\n",
    "> æ ¹æ® *DATA420-25S2 Assignment 1 (Grading)* æ•´ç†ï¼ˆAnswers 25, Reasoning 25, Tables 7, Visualizations 18, Writing 13, Coding 12ï¼‰ã€‚  \n",
    "å®Œæˆæ¯ä¸€é¡¹ä»»åŠ¡åï¼Œè¯·åœ¨å¯¹åº”æ¸…å•å¤„æ‰“å‹¾ã€‚\n",
    "\n",
    "- [ ] **Answers**ï¼šæ‰€æœ‰é—®é¢˜çš„ç­”æ¡ˆç”± **Spark è®¡ç®—** å¾—å‡ºï¼Œå•ä½æ¸…æ™°ï¼ˆrows / years / stationsï¼‰ã€‚\n",
    "- [ ] **Reasoning**ï¼šå¯¹ *æ€ä¹ˆåš & ä¸ºä»€ä¹ˆè¿™æ ·åš* ç»™å‡ºè§£é‡Šï¼ˆæ•°æ®ç»“æ„ã€ç±»å‹é€‰æ‹©ã€join ä»£ä»·ã€å¯è§†åŒ–æ–¹æ³•ç­‰ï¼‰ã€‚\n",
    "- [ ] **Tables**ï¼šæä¾›æ‰€éœ€ç»Ÿè®¡è¡¨ï¼ˆæ•°æ®é›†å¤§å°ä¸è¡Œæ•°ã€core elements è®¡æ•°ã€å›½å®¶/å·æ±‡æ€»ç­‰ï¼‰ã€‚\n",
    "- [ ] **Visualizations**ï¼šç›®å½•æ ‘ã€å¹´åº¦ daily å¤§å°ã€NZ station åœ°å›¾ã€TMIN/TMAX å­å›¾ & å…¨å›½å‡å€¼ã€2024 é™æ°´ choroplethã€‚\n",
    "- [ ] **Writing**ï¼šæŠ¥å‘Šç»“æ„ï¼ˆBackground / Processing / Analysis / Visualizations / Conclusions / Referencesï¼‰ï¼Œè¯­è¨€ç®€æ´ä¸“ä¸šï¼Œæ­£ç¡®å¼•ç”¨å¤–éƒ¨èµ„æºä¸ AI ä½¿ç”¨ã€‚\n",
    "- [ ] **Coding**ï¼šNotebook ç»“æ„æ¸…æ™°ã€æ— å¼‚å¸¸ cellã€æ³¨é‡Šå®Œå–„ã€é£æ ¼ç»Ÿä¸€ã€è¡¥å……ææ–™æœ‰åºã€‚\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6cfa6",
   "metadata": {},
   "source": [
    "## 0. ç¯å¢ƒä¸ä¼šè¯ï¼ˆEnvironment & Spark Sessionï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b6921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "global azure_account_name\n",
    "global azure_data_container_name\n",
    "global azure_user_container_name\n",
    "global azure_user_token\n",
    "\n",
    "azure_account_name = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "azure_user_token = r\"sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D\"\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(f\"fs.azure.sas.{azure_user_container_name}.{azure_account_name}.blob.core.windows.net\",  azure_user_token)\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a56f2",
   "metadata": {},
   "source": [
    "### Assignment 1 ###\n",
    "\n",
    "The code below demonstrates how to explore and load the data provided for the assignment from Azure Blob Storage and how to save any outputs that you generate to a separate user container.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The data provided for the assignment is stored in Azure Blob Storage and outputs that you generate will be stored in Azure Blob Storage as well. Hadoop and Spark can both interact with Azure Blob Storage similar to how they interact with HDFS, but where the replication and distribution is handled by Azure instead. This makes it possible to read or write data in Azure over HTTPS where the path is prefixed by `wasbs://`.\n",
    "- There are two containers, one for the data which is read only and one for any outputs that you generate,\n",
    "  - `wasbs://campus-data@madsstorage002.blob.core.windows.net/`\n",
    "  - `wasbs://campus-user@madsstorage002.blob.core.windows.net/`\n",
    "- You can use variable interpolation to insert your global username variable into paths automatically.\n",
    "  - This works for bash commands as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104ebddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: SPARK_DRIVER_BIND_ADDRESS\n",
      "25/08/01 21:36:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>jsw93 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4043\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>4</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/jsw93/spark/</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-80de32c5f2824931896179ff15cd5530</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>32</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1754040985665</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>jsw93</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>jsw93-notebook-1aa1d29864fd6b40</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16</td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1754040985490</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>jsw93 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7077</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>jsw93 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=4, executor_cores=2, worker_memory=4, master_memory=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8898d535",
   "metadata": {},
   "source": [
    "## 0.1 è·¯å¾„ä¸å·¥å…·ï¼ˆPaths & Helpersï¼‰\n",
    "\n",
    "- **è¾“å…¥ï¼ˆInput, read-onlyï¼‰**ï¼š`wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/`\n",
    "- **è¾“å‡ºï¼ˆOutput, per-userï¼‰**ï¼š`wasbs://campus-user@madsstorage002.blob.core.windows.net/<username>/`\n",
    "- **æœ¬åœ°ç¼“å­˜ï¼ˆå¯é€‰ï¼‰**ï¼šä»…å°è§„æ¨¡ä¸­é—´ç»“æœç”¨äºå›¾å½¢åŒ–ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f474f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·¯å¾„é…ç½® â€”â€” æŒ‰éœ€æ›¿æ¢ <username>\n",
    "DATA_ROOT = \"wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/\"\n",
    "USER_ROOT = \"wasbs://campus-user@madsstorage002.blob.core.windows.net/<username>/\"  # TODO: æ›¿æ¢ä¸ºä½ çš„ç”¨æˆ·å\n",
    "\n",
    "paths = {\n",
    "    \"daily\":     DATA_ROOT + \"daily/\",\n",
    "    \"stations\":  DATA_ROOT + \"stations/ghcnd-stations.txt\",\n",
    "    \"countries\": DATA_ROOT + \"countries/ghcnd-countries.txt\",\n",
    "    \"states\":    DATA_ROOT + \"states/ghcnd-states.txt\",\n",
    "    \"inventory\": DATA_ROOT + \"inventory/ghcnd-inventory.txt\",\n",
    "}\n",
    "\n",
    "paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¸¸ç”¨å¯¼å…¥\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Pandas/ç»˜å›¾ï¼ˆä»…å¯¹è¾ƒå°èšåˆç»“æœä½¿ç”¨ï¼‰\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11ae367",
   "metadata": {},
   "source": [
    "# 1. Processingï¼ˆæ•°æ®å¤„ç†ï¼‰\n",
    "\n",
    "> å¯¹ç…§ Assignment 1 â€“ Processing Q1â€“Q4ã€‚å®Œæˆåè¾“å‡ºç»Ÿè®¡è¡¨ï¼Œå¹¶ä¿å­˜ **enriched stations** åˆ° `USER_ROOT`ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf23ca",
   "metadata": {},
   "source": [
    "## 1.1 Q1: ä½¿ç”¨ `hdfs` æ¢ç´¢æ•°æ®ç»“æ„ï¼ˆData layout, compression, yearsï¼‰\n",
    "\n",
    "**ç›®æ ‡ï¼ˆAnswersï¼‰**ï¼šç›®å½•ç»“æ„ã€å‹ç¼©çŠ¶æ€ã€å¹´ä»½èŒƒå›´ã€å¤§å°ç»Ÿè®¡ã€‚  \n",
    "**Reasoning**ï¼šå‹ç¼©/éå‹ç¼©å·®å¼‚ï¼Œè§£å‹åå¤§å°ä¼°ç®—ï¼›daily éšæ—¶é—´ä½“é‡å˜åŒ–ã€‚  \n",
    "**Tables/Vis**ï¼šæ•°æ®é›†å¤§å°è¡¨ã€ç›®å½•æ ‘ã€å¹´åº¦å¤§å°å˜åŒ–å›¾ï¼ˆå¯é€‰ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650547ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‹¥åœ¨å¯ç”¨ç¯å¢ƒä¸­ï¼Œå¯è°ƒç”¨ shell å‘½ä»¤ï¼ˆæŸäº›ç¯å¢ƒéœ€å‰ç½® '!' æˆ–ä½¿ç”¨ Py4J è®¿é—® FSï¼‰ã€‚\n",
    "# è¿™é‡Œä¿ç•™å ä½ï¼šè¯·åœ¨é›†ç¾¤ç»ˆç«¯/Notebookä¸­è¿è¡Œ hdfs å‘½ä»¤å¹¶å°†ç»“æœç²˜è´´åˆ° markdown/è¡¨æ ¼ä¸­ã€‚\n",
    "# ç¤ºä¾‹ï¼š!hdfs dfs -du -h $DATA_ROOT\n",
    "\n",
    "# TODO: å°† hdfs dfs åˆ—è¡¨ä¸å°ºå¯¸ç»Ÿè®¡ç»“æœæ•´ç†æˆ Pandas DataFrame ä»¥ä¾¿åˆ¶è¡¨/ç»˜å›¾ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce296782",
   "metadata": {},
   "source": [
    "## 1.2 Q2: å®šä¹‰ schema å¹¶åŠ è½½æ•°æ®ï¼ˆCSV + å›ºå®šå®½åº¦æ–‡æœ¬ï¼‰\n",
    "\n",
    "**è¦ç‚¹**ï¼š  \n",
    "- `daily` ä¸º CSVï¼ˆç©ºå­—æ®µä¸ºç©ºï¼‰ï¼Œéœ€å®šä¹‰ schemaï¼ˆDATE/OBSERVATION_TIME è®¨è®º `StringType/DateType/TimestampType`ï¼‰ã€‚  \n",
    "- `stations/countries/states/inventory` ä¸ºå›ºå®šå®½åº¦æ–‡æœ¬ï¼Œä½¿ç”¨ `substring` æå–åˆ—ã€‚  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2a: å®šä¹‰ daily schema â€”â€” å¯æŒ‰ README/brief æè¿°è°ƒæ•´ç±»å‹\n",
    "daily_schema = T.StructType([\n",
    "    T.StructField(\"ID\", T.StringType(), True),\n",
    "    T.StructField(\"DATE\", T.StringType(), True),           # ä¹Ÿå¯åœ¨åŠ è½½å to_date\n",
    "    T.StructField(\"ELEMENT\", T.StringType(), True),\n",
    "    T.StructField(\"VALUE\", T.DoubleType(), True),\n",
    "    T.StructField(\"MEASUREMENT_FLAG\", T.StringType(), True),\n",
    "    T.StructField(\"QUALITY_FLAG\", T.StringType(), True),\n",
    "    T.StructField(\"SOURCE_FLAG\", T.StringType(), True),\n",
    "    T.StructField(\"OBSERVATION_TIME\", T.StringType(), True) # åŠ è½½åå†è§„èŒƒåŒ– HHMM\n",
    "])\n",
    "daily_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ad164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2b: åŠ è½½æœ€è¿‘ä¸€å¹´çš„ daily å­é›†ï¼ˆç¤ºä¾‹ï¼š2024ï¼‰\n",
    "latest_year = \"2024\"  # TODO: å¦‚éœ€åŠ¨æ€æ¢æµ‹ï¼Œå¯å…ˆä»ç›®å½•ä¸­è§£æå¹´ä»½\n",
    "daily_df = spark.read.csv(paths['daily'] + f\"{latest_year}.csv.gz\", header=False, schema=daily_schema)\n",
    "\n",
    "# å¯é€‰ï¼šè§„èŒƒåŒ–æ—¥æœŸ/æ—¶é—´\n",
    "daily_df = (daily_df\n",
    "            .withColumn(\"DATE\", F.to_date(\"DATE\", \"yyyyMMdd\"))\n",
    "            .withColumn(\"OBS_HH\", F.substring(\"OBSERVATION_TIME\", 1, 2).cast(\"int\"))\n",
    "            .withColumn(\"OBS_MM\", F.substring(\"OBSERVATION_TIME\", 3, 2).cast(\"int\"))\n",
    "           )\n",
    "\n",
    "daily_df.printSchema()\n",
    "daily_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ae5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2c: è§£æå›ºå®šå®½åº¦æ–‡æœ¬ï¼ˆstations/countries/states/inventoryï¼‰\n",
    "# è¯»å…¥ä¸ºä¸€åˆ— 'value' çš„ DataFrameï¼Œç„¶åç”¨ substring æå–å¯¹åº”å­—ç¬¦èŒƒå›´ã€‚ç´¢å¼•ä» 1 å¼€å§‹ã€‚\n",
    "\n",
    "stations_raw = spark.read.text(paths['stations'])\n",
    "\n",
    "stations_df = (stations_raw\n",
    "    .withColumn(\"ID\",        F.substring(\"value\", 1, 11))\n",
    "    .withColumn(\"LATITUDE\",  F.substring(\"value\", 13, 8).cast(\"double\"))\n",
    "    .withColumn(\"LONGITUDE\", F.substring(\"value\", 22, 9).cast(\"double\"))\n",
    "    .withColumn(\"ELEVATION\", F.substring(\"value\", 32, 6).cast(\"double\"))\n",
    "    .withColumn(\"STATE\",     F.substring(\"value\", 39, 2))\n",
    "    .withColumn(\"NAME\",      F.substring(\"value\", 42, 30))\n",
    "    .withColumn(\"GSN_FLAG\",  F.substring(\"value\", 73, 3))\n",
    "    .withColumn(\"HCN_CRN\",   F.substring(\"value\", 77, 3))\n",
    "    .withColumn(\"WMO_ID\",    F.substring(\"value\", 81, 5))\n",
    "    .drop(\"value\")\n",
    ")\n",
    "\n",
    "countries_df = (spark.read.text(paths['countries'])\n",
    "    .withColumn(\"CODE\", F.substring(\"value\", 1, 2))\n",
    "    .withColumn(\"COUNTRY_NAME\", F.substring(\"value\", 4, 61))\n",
    "    .drop(\"value\")\n",
    ")\n",
    "\n",
    "states_df = (spark.read.text(paths['states'])\n",
    "    .withColumn(\"CODE\", F.substring(\"value\", 1, 2))\n",
    "    .withColumn(\"STATE_NAME\", F.substring(\"value\", 4, 47))\n",
    "    .drop(\"value\")\n",
    ")\n",
    "\n",
    "inventory_df = (spark.read.text(paths['inventory'])\n",
    "    .withColumn(\"ID\",        F.substring(\"value\", 1, 11))\n",
    "    .withColumn(\"LATITUDE\",  F.substring(\"value\", 13, 8).cast(\"double\"))\n",
    "    .withColumn(\"LONGITUDE\", F.substring(\"value\", 22, 9).cast(\"double\"))\n",
    "    .withColumn(\"ELEMENT\",   F.substring(\"value\", 32, 4))\n",
    "    .withColumn(\"FIRSTYEAR\", F.substring(\"value\", 37, 4).cast(\"int\"))\n",
    "    .withColumn(\"LASTYEAR\",  F.substring(\"value\", 42, 4).cast(\"int\"))\n",
    "    .drop(\"value\")\n",
    ")\n",
    "\n",
    "stations_df.printSchema()\n",
    "countries_df.printSchema()\n",
    "states_df.printSchema()\n",
    "inventory_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321962a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2dâ€“e: è¡Œæ•°ç»Ÿè®¡\n",
    "counts = {\n",
    "    \"stations_rows\": stations_df.count(),\n",
    "    \"countries_rows\": countries_df.count(),\n",
    "    \"states_rows\": states_df.count(),\n",
    "    \"inventory_rows\": inventory_df.count(),\n",
    "    \"daily_rows_2024\": daily_df.count()\n",
    "}\n",
    "counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7df927",
   "metadata": {},
   "source": [
    "## 1.3 Q3: æ„å»º enriched stationsï¼ˆcountry/state/inventory èšåˆï¼‰\n",
    "\n",
    "**ç›®æ ‡**ï¼šæå–å›½å®¶ä»£ç ã€LEFT JOIN countries & statesï¼›ç»Ÿè®¡æ¯ç«™ first/last yearã€core/other å…ƒç´ æ•°é‡ï¼›ä¿å­˜ enriched è¡¨ã€‚  \n",
    "**ä¼˜åŒ–å»ºè®®**ï¼šå…ˆå¯¹ `inventory` æŒ‰å…ƒç´ ç±»åˆ«è¿‡æ»¤å† joinï¼›é¿å…æ— å¿…è¦çš„å®½è¡¨ç‰©åŒ–ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8cb682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3a: ä» station ID æå–å›½å®¶ä»£ç \n",
    "stations_enriched = stations_df.withColumn(\"COUNTRY_CODE\", F.substring(\"ID\", 1, 2))\n",
    "\n",
    "# Q3b: LEFT JOIN countries\n",
    "stations_enriched = (stations_enriched\n",
    "    .join(countries_df.withColumnRenamed(\"CODE\", \"COUNTRY_CODE\"), on=\"COUNTRY_CODE\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Q3c: LEFT JOIN statesï¼ˆä»… US é€‚ç”¨ï¼‰\n",
    "stations_enriched = (stations_enriched\n",
    "    .join(states_df.withColumnRenamed(\"CODE\", \"STATE\"), on=\"STATE\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Q3d: inventory èšåˆ\n",
    "core_elements = F.array(F.lit(\"TMAX\"), F.lit(\"TMIN\"), F.lit(\"PRCP\"), F.lit(\"SNOW\"), F.lit(\"SNWD\"))\n",
    "\n",
    "inv_by_station = (inventory_df\n",
    "    .groupBy(\"ID\")\n",
    "    .agg(\n",
    "        F.min(\"FIRSTYEAR\").alias(\"FIRSTYEAR_ANY\"),\n",
    "        F.max(\"LASTYEAR\").alias(\"LASTYEAR_ANY\"),\n",
    "        F.countDistinct(\"ELEMENT\").alias(\"N_ELEMENTS\"),\n",
    "        F.sum(F.when(F.col(\"ELEMENT\").isin(\"TMAX\",\"TMIN\",\"PRCP\",\"SNOW\",\"SNWD\"), 1).otherwise(0)).alias(\"N_CORE_ELEMENTS\")\n",
    "    )\n",
    ")\n",
    "\n",
    "stations_enriched = (stations_enriched\n",
    "    .join(inv_by_station.withColumnRenamed(\"ID\", \"ID\"), on=\"ID\", how=\"left\")\n",
    ")\n",
    "\n",
    "stations_enriched.printSchema()\n",
    "stations_enriched.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909658bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3e: ä¿å­˜ enriched stationsï¼ˆå»ºè®® parquetï¼‰\n",
    "(stations_enriched\n",
    " .write.mode(\"overwrite\")\n",
    " .parquet(USER_ROOT + \"enriched_stations.parquet\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d59284",
   "metadata": {},
   "source": [
    "## 1.4 Q4: æ£€æŸ¥ daily ç¼ºå¤±çš„ stations\n",
    "\n",
    "**ç›®æ ‡**ï¼šæ‰¾å‡ºåœ¨ `stations` ä¸­ä½†å®Œå…¨ä¸å‡ºç°åœ¨ `daily` çš„ç«™ç‚¹æ•°ã€‚  \n",
    "**æç¤º**ï¼šé¿å…å…¨é‡ joinï¼Œå¯å…ˆè·å¾—æœ€è¿‘ä¸€å¹´çš„ station ID å­é›†æˆ–ç”¨ distinct station ID æ˜ å°„ + broadcast å°è¡¨ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc7b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå– 2024 å¹´ daily ä¸­å‡ºç°è¿‡çš„ station IDï¼ˆå¯æ ¹æ®éœ€è¦æ‰©å±•å¹´ä»½èŒƒå›´ï¼‰\n",
    "daily_station_ids_2024 = daily_df.select(\"ID\").distinct().withColumnRenamed(\"ID\", \"ID_IN_DAILY\")\n",
    "\n",
    "# ä½¿ç”¨ left anti join æ‰¾å‡ºä»æœªå‡ºç°åœ¨ daily_2024 çš„ç«™\n",
    "missing_in_daily = (stations_df\n",
    "    .join(daily_station_ids_2024, stations_df.ID == daily_station_ids_2024.ID_IN_DAILY, how=\"left_anti\")\n",
    ")\n",
    "\n",
    "missing_count = missing_in_daily.count()\n",
    "missing_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a238f",
   "metadata": {},
   "source": [
    "# 2. Analysisï¼ˆåˆ†æé—®ç­”ï¼‰\n",
    "\n",
    "> å¯¹ç…§ Assignment 1 â€“ Analysis Q1â€“Q3ã€‚å¼ºè°ƒ **æ–¹æ³•è§£é‡Šï¼ˆReasoningï¼‰** ä¸ **æ•ˆç‡**ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc874a2",
   "metadata": {},
   "source": [
    "## 2.1 Q1: ç«™ç‚¹æ¦‚å†µç»Ÿè®¡ï¼ˆæ€»æ•°ã€2025æ´»è·ƒã€ç½‘ç»œå½’å±ã€å—åŠçƒã€ç¾å›½å±åœ°ã€å›½å®¶/å·ç»Ÿè®¡ï¼‰\n",
    "\n",
    "**è¦ç‚¹**ï¼š  \n",
    "- 2025 æ´»è·ƒï¼š`LASTYEAR_ANY >= 2025` æˆ–ç»“åˆ daily 2025 æ˜¯å¦æœ‰è§‚æµ‹ã€‚  \n",
    "- å—åŠçƒï¼š`LATITUDE < 0`ã€‚  \n",
    "- ç½‘ç»œå­—æ®µï¼š`GSN_FLAG` / `HCN_CRN`ã€‚  \n",
    "- å›½å®¶/å·åˆ†å¸ƒï¼šç»Ÿè®¡å¹¶ä¿å­˜ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b19bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€»ç«™ç‚¹æ•°\n",
    "total_stations = stations_df.count()\n",
    "\n",
    "# 2025 æ´»è·ƒï¼ˆåŸºäº inventory æ±‡æ€»ï¼‰\n",
    "active_2025 = stations_enriched.filter(F.col(\"LASTYEAR_ANY\") >= 2025).count()\n",
    "\n",
    "# ç½‘ç»œå½’å±ç»Ÿè®¡ï¼ˆç¤ºä¾‹ï¼šæ˜¯å¦åœ¨ GSN/HCN/CRNï¼Œå…·ä½“å–å€¼éœ€æ ¹æ®æ•°æ®å–æ ·éªŒè¯ï¼‰\n",
    "network_counts = (stations_enriched\n",
    "    .select(\n",
    "        F.when(F.col(\"GSN_FLAG\").isNotNull() & (F.col(\"GSN_FLAG\") != \"\"), 1).otherwise(0).alias(\"is_GSN\"),\n",
    "        F.when(F.col(\"HCN_CRN\").contains(\"HCN\"), 1).otherwise(0).alias(\"is_HCN\"),\n",
    "        F.when(F.col(\"HCN_CRN\").contains(\"CRN\"), 1).otherwise(0).alias(\"is_CRN\"),\n",
    "    )\n",
    "    .agg(F.sum(\"is_GSN\").alias(\"GSN\"),\n",
    "         F.sum(\"is_HCN\").alias(\"HCN\"),\n",
    "         F.sum(\"is_CRN\").alias(\"CRN\"))\n",
    ")\n",
    "\n",
    "# å—åŠçƒç«™ç‚¹æ•°\n",
    "southern_hemisphere = stations_df.filter(F.col(\"LATITUDE\") < 0).count()\n",
    "\n",
    "# ç¾å›½å±åœ°ï¼ˆå›½å®¶åä¸­åŒ…å« United States ä½†ä¸ç­‰äº United Statesï¼‰\n",
    "us_territories = stations_enriched.filter(\n",
    "    (F.col(\"COUNTRY_NAME\").contains(\"United States\")) & (F.col(\"COUNTRY_NAME\") != \"United States\")\n",
    ").count()\n",
    "\n",
    "# æŒ‰å›½å®¶ä¸å·ç»Ÿè®¡å¹¶ä¿å­˜\n",
    "by_country = (stations_enriched.groupBy(\"COUNTRY_CODE\", \"COUNTRY_NAME\").count())\n",
    "by_state = (stations_enriched.filter(F.col(\"STATE\").isNotNull())\n",
    "            .groupBy(\"STATE\",\"STATE_NAME\").count())\n",
    "\n",
    "by_country.write.mode(\"overwrite\").parquet(USER_ROOT + \"stations_by_country.parquet\")\n",
    "by_state.write.mode(\"overwrite\").parquet(USER_ROOT + \"stations_by_state.parquet\")\n",
    "\n",
    "(total_stations, active_2025, network_counts.collect(), southern_hemisphere, us_territories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a9beef",
   "metadata": {},
   "source": [
    "## 2.2 Q2: UDF è®¡ç®—åœ°ç†è·ç¦»ï¼ˆHaversineï¼‰å¹¶åœ¨ NZ ç«™ç‚¹ä¸¤ä¸¤é…å¯¹\n",
    "\n",
    "**è¦ç‚¹**ï¼š  \n",
    "- ä»…å¯¹ **æ–°è¥¿å…°** ç«™ç‚¹å¯¹å­åš pairwise è®¡ç®—ï¼ˆå­é›†æ›´é«˜æ•ˆï¼‰ã€‚  \n",
    "- ä½¿ç”¨ **Haversine formula** è®¡ç®—çƒé¢è·ç¦»ï¼ˆå•ä½å…¬é‡Œï¼‰ã€‚  \n",
    "- æ‰¾å‡ºæœ€è¿‘ä¸¤ç«™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€‰å–æ–°è¥¿å…°ï¼ˆNew Zealandï¼‰ç«™ç‚¹\n",
    "nz_stations = stations_enriched.filter(F.col(\"COUNTRY_NAME\").contains(\"New Zealand\"))                                .select(\"ID\",\"NAME\",\"LATITUDE\",\"LONGITUDE\")\n",
    "\n",
    "# äº§ç”Ÿä¸¤ä¸¤ç»„åˆï¼ˆSELF CROSS JOINï¼Œæ³¨æ„è§„æ¨¡ï¼›å¦‚éœ€è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå¯é‡‡æ ·æˆ–åŸºäºç½‘æ ¼ç´¢å¼•ï¼‰\n",
    "left = nz_stations.select(\n",
    "    F.col(\"ID\").alias(\"ID_A\"),\n",
    "    F.col(\"NAME\").alias(\"NAME_A\"),\n",
    "    F.col(\"LATITUDE\").alias(\"LAT_A\"),\n",
    "    F.col(\"LONGITUDE\").alias(\"LON_A\"),\n",
    ")\n",
    "right = nz_stations.select(\n",
    "    F.col(\"ID\").alias(\"ID_B\"),\n",
    "    F.col(\"NAME\").alias(\"NAME_B\"),\n",
    "    F.col(\"LATITUDE\").alias(\"LAT_B\"),\n",
    "    F.col(\"LONGITUDE\").alias(\"LON_B\"),\n",
    ")\n",
    "\n",
    "pairs = left.crossJoin(right).filter(F.col(\"ID_A\") < F.col(\"ID_B\"))\n",
    "\n",
    "# Haversine UDF\n",
    "from math import radians, sin, cos, asin, sqrt\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0088  # mean Earth radius in km\n",
    "    phi1, phi2 = radians(lat1), radians(lat2)\n",
    "    dphi = radians(lat2 - lat1)\n",
    "    dlambda = radians(lon2 - lon1)\n",
    "    a = sin(dphi/2)**2 + cos(phi1)*cos(phi2)*sin(dlambda/2)**2\n",
    "    c = 2*asin(sqrt(a))\n",
    "    return float(R*c)\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "haversine_udf = udf(haversine_km, T.DoubleType())\n",
    "\n",
    "pairs = pairs.withColumn(\"DIST_KM\", haversine_udf(\"LAT_A\",\"LON_A\",\"LAT_B\",\"LON_B\"))\n",
    "\n",
    "closest_pair = pairs.orderBy(F.col(\"DIST_KM\").asc()).limit(1)\n",
    "closest_pair.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db98a84",
   "metadata": {},
   "source": [
    "## 2.3 Q3: Core elements ç»Ÿè®¡ä¸ TMAX æ— é…å¯¹ TMIN æ•°é‡\n",
    "\n",
    "**è¦ç‚¹**ï¼š  \n",
    "- Core elements = {TMAX, TMIN, PRCP, SNOW, SNWD}ã€‚  \n",
    "- ç»Ÿè®¡å„å…ƒç´ è§‚æµ‹æ•°ï¼›ç»Ÿè®¡æ— é…å¯¹ï¼ˆTMAX ä¸”åŒç«™åŒæ—¥æ—  TMINï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c03ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»…æ ¸å¿ƒè¦ç´ \n",
    "core_elems = [\"TMAX\",\"TMIN\",\"PRCP\",\"SNOW\",\"SNWD\"]\n",
    "daily_core = daily_df.filter(F.col(\"ELEMENT\").isin(core_elems))\n",
    "\n",
    "# å„å…ƒç´ è§‚æµ‹æ•°\n",
    "elem_counts = daily_core.groupBy(\"ELEMENT\").count()\n",
    "elem_counts.show()\n",
    "\n",
    "# TMAX æ— é…å¯¹ TMINï¼šåŸºäºï¼ˆID, DATEï¼‰\n",
    "tmax = daily_df.filter(F.col(\"ELEMENT\")==\"TMAX\").select(F.col(\"ID\").alias(\"ID_T\"), F.col(\"DATE\").alias(\"DATE_T\"))\n",
    "tmin = daily_df.filter(F.col(\"ELEMENT\")==\"TMIN\").select(F.col(\"ID\").alias(\"ID_N\"), F.col(\"DATE\").alias(\"DATE_N\"))\n",
    "\n",
    "tmax_no_tmin = (tmax.join(tmin, (tmax.ID_T==tmin.ID_N) & (tmax.DATE_T==tmin.DATE_N), how=\"left_anti\"))\n",
    "missing_pairs_count = tmax_no_tmin.count()\n",
    "\n",
    "# å‚ä¸è¿™äº›è§‚æµ‹çš„å”¯ä¸€ç«™ç‚¹æ•°\n",
    "unique_stations_missing = tmax_no_tmin.select(\"ID_T\").distinct().count()\n",
    "\n",
    "(missing_pairs_count, unique_stations_missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938db02",
   "metadata": {},
   "source": [
    "# 3. Visualizationsï¼ˆå¯è§†åŒ–ï¼‰\n",
    "\n",
    "> å¯¹ç…§ Assignment 1 â€“ Visualization Q1â€“Q2ã€‚å…ˆåœ¨ Spark ä¾§èšåˆï¼Œå† `.toPandas()` ç»˜å›¾ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7110d771",
   "metadata": {},
   "source": [
    "## 3.1 Q1: New Zealand â€” TMIN/TMAX æ—¶é—´åºåˆ—ï¼ˆç«™ç‚¹å­å›¾ + å…¨å›½å¹³å‡ï¼‰\n",
    "\n",
    "**æµç¨‹**ï¼š  \n",
    "1) è¿‡æ»¤ NZ ç«™ç‚¹ ID â†’ è¿‡æ»¤ daily ä¸­ TMIN/TMAX â†’ é€‰æ‹©åˆé€‚çš„æ—¶é—´èšåˆï¼ˆå¦‚æœˆå¹³å‡ï¼‰ã€‚  \n",
    "2) ç«™ç‚¹çº§åˆ«ï¼šæ¯ç«™ä¸€å¹…å­å›¾ï¼ˆsubplotï¼‰ã€‚  \n",
    "3) å…¨å›½ï¼šåˆå¹¶åä½œä¸€å¹…å¤§å›¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3889fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·å– NZ ç«™ç‚¹ ID é›†åˆ\n",
    "nz_ids_df = nz_stations.select(\"ID\").distinct()\n",
    "daily_nz_tm = (daily_df\n",
    "    .join(nz_ids_df, on=\"ID\", how=\"inner\")\n",
    "    .filter(F.col(\"ELEMENT\").isin(\"TMIN\",\"TMAX\"))\n",
    ")\n",
    "\n",
    "# é€‰æ‹©æœˆå¹³å‡ä½œä¸ºå¹³æ»‘çº§åˆ«\n",
    "daily_nz_tm = daily_nz_tm.withColumn(\"YEAR\", F.year(\"DATE\")).withColumn(\"MONTH\", F.month(\"DATE\"))\n",
    "monthly_nz = (daily_nz_tm\n",
    "    .groupBy(\"ID\",\"ELEMENT\",\"YEAR\",\"MONTH\")\n",
    "    .agg(F.avg(\"VALUE\").alias(\"AVG_VALUE\"))\n",
    ")\n",
    "\n",
    "# æ”¶é›†ä¸º Pandas åç»˜å›¾ï¼ˆæ³¨æ„è§„æ¨¡æ§åˆ¶ï¼‰\n",
    "pdf = monthly_nz.toPandas()\n",
    "\n",
    "# TODO: ä½¿ç”¨ matplotlib/plotly ç”Ÿæˆ subplotï¼ˆæ¯ç«™ä¸€å­å›¾ï¼‰ä¸å…¨å›½å¹³å‡æ›²çº¿ã€‚\n",
    "# æ³¨æ„ï¼šVALUE å•ä½å¯èƒ½éœ€æ¢ç®—/æ ‡æ³¨ï¼›ç¼ºå£ï¼ˆç¼ºå¤±æœˆä»½ï¼‰éœ€æ˜¾å¼æ˜¾ç¤ºã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edda962",
   "metadata": {},
   "source": [
    "## 3.2 Q2: å…¨çƒå¹´åº¦é™æ°´ï¼ˆPRCPï¼‰â€” å›½å®¶çº§ Choroplethï¼ˆ2024ï¼‰\n",
    "\n",
    "**æµç¨‹**ï¼š  \n",
    "1) ä» daily è¿‡æ»¤ PRCP â†’ æŒ‰å›½å®¶+å¹´ä»½èšåˆ â†’ è®¡ç®—å¹´åº¦å¹³å‡æ—¥é™æ°´ã€‚  \n",
    "2) ä¿å­˜ç»Ÿè®¡è¡¨åˆ° `USER_ROOT`ã€‚  \n",
    "3) ä¸ `geopandas` æˆ– `plotly` çš„å›½å®¶åç§°åŒ¹é…ï¼Œç»˜åˆ¶ 2024 å¹´ choroplethã€‚  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¡ç®—å¹´åº¦å›½å®¶å¹³å‡æ—¥é™æ°´ï¼ˆç¤ºä¾‹ï¼š2024 å¹´ï¼‰\n",
    "year_target = 2024\n",
    "daily_prcp = daily_df.filter(F.col(\"ELEMENT\")==\"PRCP\").withColumn(\"YEAR\", F.year(\"DATE\"))\n",
    "\n",
    "# å°†ç«™ç‚¹ä¸å›½å®¶ joinï¼ˆä½¿ç”¨ enriched_stationsï¼‰\n",
    "daily_prcp_country = (daily_prcp\n",
    "    .join(stations_enriched.select(\"ID\",\"COUNTRY_CODE\",\"COUNTRY_NAME\"), on=\"ID\", how=\"left\")\n",
    "    .filter(F.col(\"YEAR\")==year_target)\n",
    ")\n",
    "\n",
    "prcp_by_country_year = (daily_prcp_country\n",
    "    .groupBy(\"COUNTRY_CODE\",\"COUNTRY_NAME\",\"YEAR\")\n",
    "    .agg(F.avg(\"VALUE\").alias(\"AVG_DAILY_PRCP\"))\n",
    ")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "(prcp_by_country_year\n",
    " .write.mode(\"overwrite\")\n",
    " .parquet(USER_ROOT + f\"prcp_by_country_year_{year_target}.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOï¼ˆå¯è§†åŒ–ï¼‰ï¼š\n",
    "# 1) prcp_by_country_year_{year}.parquet è¯»å–ä¸º Pandas\n",
    "# 2) ä¸è‡ªç„¶åœ°ç†å›½å®¶è¾¹ç•Œï¼ˆgeopandas è‡ªå¸¦æˆ– naturalearth_lowresï¼‰åŒ¹é…\n",
    "# 3) ç»˜åˆ¶ choroplethï¼ˆæ³¨æ„ï¼šæŠ•å½±é€‰æ‹©ã€é…è‰²ã€ç¼ºå¤±å›½å®¶å¤„ç†ã€å¼‚å¸¸å€¼æ³¨è®°ï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c62706f",
   "metadata": {},
   "source": [
    "# 4. æŠ¥å‘Šå†™ä½œï¼ˆWritingï¼‰ä¸å¯¼å‡ºï¼ˆExportï¼‰\n",
    "\n",
    "- åœ¨æ¯ä¸ªé—®é¢˜å®Œæˆåï¼Œè®°å½•ï¼š**Methodologyï¼ˆæ–¹æ³•ï¼‰** â†’ **Resultsï¼ˆç»“æœï¼‰** â†’ **Reasoningï¼ˆè§£é‡Šï¼‰**ã€‚  \n",
    "- å°†è¡¨æ ¼å’Œå›¾å½¢å¯¼å‡ºåˆ° supplementary materialï¼ˆzipï¼‰ã€‚  \n",
    "- åœ¨æŠ¥å‘Šæ­£æ–‡ä¸­å¼•ç”¨å›¾è¡¨ï¼ˆFigure X / Table Yï¼‰å¹¶è¯´æ˜å«ä¹‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f884407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯é€‰ï¼šå°†å…³é”®ç»Ÿè®¡å¯¼å‡ºä¸º CSV ä¾¿äºåœ¨æŠ¥å‘Šä¸­å¼•ç”¨\n",
    "# ç¤ºä¾‹ï¼šelem_counts.toPandas().to_csv('/mnt/data/core_element_counts.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d2b9ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç»“æŸï¼ˆFinishï¼‰\n",
    "\n",
    "- è¯·ç¡®ä¿ï¼šæ‰€æœ‰è·¯å¾„æ­£ç¡®ã€è¾“å‡ºå·²å†™å…¥ `USER_ROOT`ã€Notebook æ— æŠ¥é”™ã€‚  \n",
    "- æäº¤ï¼šæŠ¥å‘Š PDFï¼ˆ3,000â€“5,000 å­—ï¼‰+ supplementary zipï¼ˆä»£ç ã€å›¾ã€è„šæœ¬ï¼›ä¸åŒ…å«æ•°æ®è¾“å‡ºï¼‰ã€‚\n",
    "\n",
    "> Good luck! ğŸš€\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
