{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b899f34",
   "metadata": {},
   "source": [
    "# DATA420-25S2 Assignment 1 — GHCN Data Analysis using Spark  \n",
    "**Notebook Framework (Scaffold)** · 2025-08-14\n",
    "\n",
    "> 本 Notebook 作为 **可运行框架** 使用：按题目顺序预置了代码单元与输出占位，并内置 *grading checklist*，以便你边学 Lecture 9–12 边完成作业。\n",
    "\n",
    "**作者（Author）**：Yu Xia  \n",
    "**学号（Student ID）**：62380486\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36bbe0",
   "metadata": {},
   "source": [
    "## ✅ Grading Checklist（对标评分要点）\n",
    "\n",
    "> 根据 *DATA420-25S2 Assignment 1 (Grading)* 整理（Answers 25, Reasoning 25, Tables 7, Visualizations 18, Writing 13, Coding 12）。  \n",
    "完成每一项任务后，请在对应清单处打勾。\n",
    "\n",
    "- [ ] **Answers**：所有问题的答案由 **Spark 计算** 得出，单位清晰（rows / years / stations）。\n",
    "- [ ] **Reasoning**：对 *怎么做 & 为什么这样做* 给出解释（数据结构、类型选择、join 代价、可视化方法等）。\n",
    "- [ ] **Tables**：提供所需统计表（数据集大小与行数、core elements 计数、国家/州汇总等）。\n",
    "- [ ] **Visualizations**：目录树、年度 daily 大小、NZ station 地图、TMIN/TMAX 子图 & 全国均值、2024 降水 choropleth。\n",
    "- [ ] **Writing**：报告结构（Background / Processing / Analysis / Visualizations / Conclusions / References），语言简洁专业，正确引用外部资源与 AI 使用。\n",
    "- [ ] **Coding**：Notebook 结构清晰、无异常 cell、注释完善、风格统一、补充材料有序。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6cfa6",
   "metadata": {},
   "source": [
    "## 0. 环境与会话（Environment & Spark Session）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b6921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "global azure_account_name\n",
    "global azure_data_container_name\n",
    "global azure_user_container_name\n",
    "global azure_user_token\n",
    "\n",
    "azure_account_name = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "azure_user_token = r\"sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D\"\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(f\"fs.azure.sas.{azure_user_container_name}.{azure_account_name}.blob.core.windows.net\",  azure_user_token)\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a56f2",
   "metadata": {},
   "source": [
    "### Assignment 1 ###\n",
    "\n",
    "The code below demonstrates how to explore and load the data provided for the assignment from Azure Blob Storage and how to save any outputs that you generate to a separate user container.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The data provided for the assignment is stored in Azure Blob Storage and outputs that you generate will be stored in Azure Blob Storage as well. Hadoop and Spark can both interact with Azure Blob Storage similar to how they interact with HDFS, but where the replication and distribution is handled by Azure instead. This makes it possible to read or write data in Azure over HTTPS where the path is prefixed by `wasbs://`.\n",
    "- There are two containers, one for the data which is read only and one for any outputs that you generate,\n",
    "  - `wasbs://campus-data@madsstorage002.blob.core.windows.net/`\n",
    "  - `wasbs://campus-user@madsstorage002.blob.core.windows.net/`\n",
    "- You can use variable interpolation to insert your global username variable into paths automatically.\n",
    "  - This works for bash commands as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104ebddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: SPARK_DRIVER_BIND_ADDRESS\n",
      "25/08/01 21:36:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>jsw93 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4043\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>4</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/jsw93/spark/</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-80de32c5f2824931896179ff15cd5530</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>32</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1754040985665</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>jsw93</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>jsw93-notebook-1aa1d29864fd6b40</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16</td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1754040985490</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>jsw93 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7077</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>jsw93 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=4, executor_cores=2, worker_memory=4, master_memory=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8898d535",
   "metadata": {},
   "source": [
    "## 0.1 路径与工具（Paths & Helpers）\n",
    "\n",
    "- **输入（Input, read-only）**：`wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/`\n",
    "- **输出（Output, per-user）**：`wasbs://campus-user@madsstorage002.blob.core.windows.net/<username>/`\n",
    "- **本地缓存（可选）**：仅小规模中间结果用于图形化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f474f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 路径配置 —— 按需替换 <username>\n",
    "DATA_ROOT = \"wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/\"\n",
    "USER_ROOT = \"wasbs://campus-user@madsstorage002.blob.core.windows.net/<username>/\"  # TODO: 替换为你的用户名\n",
    "\n",
    "paths = {\n",
    "    \"daily\":     DATA_ROOT + \"daily/\",\n",
    "    \"stations\":  DATA_ROOT + \"stations/ghcnd-stations.txt\",\n",
    "    \"countries\": DATA_ROOT + \"countries/ghcnd-countries.txt\",\n",
    "    \"states\":    DATA_ROOT + \"states/ghcnd-states.txt\",\n",
    "    \"inventory\": DATA_ROOT + \"inventory/ghcnd-inventory.txt\",\n",
    "}\n",
    "\n",
    "paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常用导入\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Pandas/绘图（仅对较小聚合结果使用）\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11ae367",
   "metadata": {},
   "source": [
    "# 1. Processing（数据处理）\n",
    "\n",
    "> 对照 Assignment 1 – Processing Q1–Q4。完成后输出统计表，并保存 **enriched stations** 到 `USER_ROOT`。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf23ca",
   "metadata": {},
   "source": [
    "## 1.1 Q1: 使用 `hdfs` 探索数据结构（Data layout, compression, years）\n",
    "\n",
    "**目标（Answers）**：目录结构、压缩状态、年份范围、大小统计。  \n",
    "**Reasoning**：压缩/非压缩差异，解压后大小估算；daily 随时间体量变化。  \n",
    "**Tables/Vis**：数据集大小表、目录树、年度大小变化图（可选）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650547ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若在可用环境中，可调用 shell 命令（某些环境需前置 '!' 或使用 Py4J 访问 FS）。\n",
    "# 这里保留占位：请在集群终端/Notebook中运行 hdfs 命令并将结果粘贴到 markdown/表格中。\n",
    "# 示例：!hdfs dfs -du -h $DATA_ROOT\n",
    "\n",
    "# TODO: 将 hdfs dfs 列表与尺寸统计结果整理成 Pandas DataFrame 以便制表/绘图。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce296782",
   "metadata": {},
   "source": [
    "## 1.2 Q2: 定义 schema 并加载数据（CSV + 固定宽度文本）\n",
    "\n",
    "**要点**：  \n",
    "- `daily` 为 CSV（空字段为空），需定义 schema（DATE/OBSERVATION_TIME 讨论 `StringType/DateType/TimestampType`）。  \n",
    "- `stations/countries/states/inventory` 为固定宽度文本，使用 `substring` 提取列。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2a: 定义 daily schema —— 可按 README/brief 描述调整类型\n",
    "daily_schema = T.StructType([\n",
    "    T.StructField(\"ID\", T.StringType(), True),\n",
    "    T.StructField(\"DATE\", T.StringType(), True),           # 也可在加载后 to_date\n",
    "    T.StructField(\"ELEMENT\", T.StringType(), True),\n",
    "    T.StructField(\"VALUE\", T.DoubleType(), True),\n",
    "    T.StructField(\"MEASUREMENT_FLAG\", T.StringType(), True),\n",
    "    T.StructField(\"QUALITY_FLAG\", T.StringType(), True),\n",
    "    T.StructField(\"SOURCE_FLAG\", T.StringType(), True),\n",
    "    T.StructField(\"OBSERVATION_TIME\", T.StringType(), True) # 加载后再规范化 HHMM\n",
    "])\n",
    "daily_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ad164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2b: 加载最近一年的 daily 子集（示例：2024）\n",
    "latest_year = \"2024\"  # TODO: 如需动态探测，可先从目录中解析年份\n",
    "daily_df = spark.read.csv(paths['daily'] + f\"{latest_year}.csv.gz\", header=False, schema=daily_schema)\n",
    "\n",
    "# 可选：规范化日期/时间\n",
    "daily_df = (daily_df\n",
    "            .withColumn(\"DATE\", F.to_date(\"DATE\", \"yyyyMMdd\"))\n",
    "            .withColumn(\"OBS_HH\", F.substring(\"OBSERVATION_TIME\", 1, 2).cast(\"int\"))\n",
    "            .withColumn(\"OBS_MM\", F.substring(\"OBSERVATION_TIME\", 3, 2).cast(\"int\"))\n",
    "           )\n",
    "\n",
    "daily_df.printSchema()\n",
    "daily_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ae5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2c: 解析固定宽度文本（stations/countries/states/inventory）\n",
    "# 读入为一列 'value' 的 DataFrame，然后用 substring 提取对应字符范围。索引从 1 开始。\n",
    "\n",
    "stations_raw = spark.read.text(paths['stations'])\n",
    "\n",
    "stations_df = (stations_raw\n",
    "    .withColumn(\"ID\",        F.substring(\"value\", 1, 11))\n",
    "    .withColumn(\"LATITUDE\",  F.substring(\"value\", 13, 8).cast(\"double\"))\n",
    "    .withColumn(\"LONGITUDE\", F.substring(\"value\", 22, 9).cast(\"double\"))\n",
    "    .withColumn(\"ELEVATION\", F.substring(\"value\", 32, 6).cast(\"double\"))\n",
    "    .withColumn(\"STATE\",     F.substring(\"value\", 39, 2))\n",
    "    .withColumn(\"NAME\",      F.substring(\"value\", 42, 30))\n",
    "    .withColumn(\"GSN_FLAG\",  F.substring(\"value\", 73, 3))\n",
    "    .withColumn(\"HCN_CRN\",   F.substring(\"value\", 77, 3))\n",
    "    .withColumn(\"WMO_ID\",    F.substring(\"value\", 81, 5))\n",
    "    .drop(\"value\")\n",
    ")\n",
    "\n",
    "countries_df = (spark.read.text(paths['countries'])\n",
    "    .withColumn(\"CODE\", F.substring(\"value\", 1, 2))\n",
    "    .withColumn(\"COUNTRY_NAME\", F.substring(\"value\", 4, 61))\n",
    "    .drop(\"value\")\n",
    ")\n",
    "\n",
    "states_df = (spark.read.text(paths['states'])\n",
    "    .withColumn(\"CODE\", F.substring(\"value\", 1, 2))\n",
    "    .withColumn(\"STATE_NAME\", F.substring(\"value\", 4, 47))\n",
    "    .drop(\"value\")\n",
    ")\n",
    "\n",
    "inventory_df = (spark.read.text(paths['inventory'])\n",
    "    .withColumn(\"ID\",        F.substring(\"value\", 1, 11))\n",
    "    .withColumn(\"LATITUDE\",  F.substring(\"value\", 13, 8).cast(\"double\"))\n",
    "    .withColumn(\"LONGITUDE\", F.substring(\"value\", 22, 9).cast(\"double\"))\n",
    "    .withColumn(\"ELEMENT\",   F.substring(\"value\", 32, 4))\n",
    "    .withColumn(\"FIRSTYEAR\", F.substring(\"value\", 37, 4).cast(\"int\"))\n",
    "    .withColumn(\"LASTYEAR\",  F.substring(\"value\", 42, 4).cast(\"int\"))\n",
    "    .drop(\"value\")\n",
    ")\n",
    "\n",
    "stations_df.printSchema()\n",
    "countries_df.printSchema()\n",
    "states_df.printSchema()\n",
    "inventory_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321962a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2d–e: 行数统计\n",
    "counts = {\n",
    "    \"stations_rows\": stations_df.count(),\n",
    "    \"countries_rows\": countries_df.count(),\n",
    "    \"states_rows\": states_df.count(),\n",
    "    \"inventory_rows\": inventory_df.count(),\n",
    "    \"daily_rows_2024\": daily_df.count()\n",
    "}\n",
    "counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7df927",
   "metadata": {},
   "source": [
    "## 1.3 Q3: 构建 enriched stations（country/state/inventory 聚合）\n",
    "\n",
    "**目标**：提取国家代码、LEFT JOIN countries & states；统计每站 first/last year、core/other 元素数量；保存 enriched 表。  \n",
    "**优化建议**：先对 `inventory` 按元素类别过滤再 join；避免无必要的宽表物化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8cb682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3a: 从 station ID 提取国家代码\n",
    "stations_enriched = stations_df.withColumn(\"COUNTRY_CODE\", F.substring(\"ID\", 1, 2))\n",
    "\n",
    "# Q3b: LEFT JOIN countries\n",
    "stations_enriched = (stations_enriched\n",
    "    .join(countries_df.withColumnRenamed(\"CODE\", \"COUNTRY_CODE\"), on=\"COUNTRY_CODE\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Q3c: LEFT JOIN states（仅 US 适用）\n",
    "stations_enriched = (stations_enriched\n",
    "    .join(states_df.withColumnRenamed(\"CODE\", \"STATE\"), on=\"STATE\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Q3d: inventory 聚合\n",
    "core_elements = F.array(F.lit(\"TMAX\"), F.lit(\"TMIN\"), F.lit(\"PRCP\"), F.lit(\"SNOW\"), F.lit(\"SNWD\"))\n",
    "\n",
    "inv_by_station = (inventory_df\n",
    "    .groupBy(\"ID\")\n",
    "    .agg(\n",
    "        F.min(\"FIRSTYEAR\").alias(\"FIRSTYEAR_ANY\"),\n",
    "        F.max(\"LASTYEAR\").alias(\"LASTYEAR_ANY\"),\n",
    "        F.countDistinct(\"ELEMENT\").alias(\"N_ELEMENTS\"),\n",
    "        F.sum(F.when(F.col(\"ELEMENT\").isin(\"TMAX\",\"TMIN\",\"PRCP\",\"SNOW\",\"SNWD\"), 1).otherwise(0)).alias(\"N_CORE_ELEMENTS\")\n",
    "    )\n",
    ")\n",
    "\n",
    "stations_enriched = (stations_enriched\n",
    "    .join(inv_by_station.withColumnRenamed(\"ID\", \"ID\"), on=\"ID\", how=\"left\")\n",
    ")\n",
    "\n",
    "stations_enriched.printSchema()\n",
    "stations_enriched.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909658bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3e: 保存 enriched stations（建议 parquet）\n",
    "(stations_enriched\n",
    " .write.mode(\"overwrite\")\n",
    " .parquet(USER_ROOT + \"enriched_stations.parquet\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d59284",
   "metadata": {},
   "source": [
    "## 1.4 Q4: 检查 daily 缺失的 stations\n",
    "\n",
    "**目标**：找出在 `stations` 中但完全不出现在 `daily` 的站点数。  \n",
    "**提示**：避免全量 join，可先获得最近一年的 station ID 子集或用 distinct station ID 映射 + broadcast 小表。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc7b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取 2024 年 daily 中出现过的 station ID（可根据需要扩展年份范围）\n",
    "daily_station_ids_2024 = daily_df.select(\"ID\").distinct().withColumnRenamed(\"ID\", \"ID_IN_DAILY\")\n",
    "\n",
    "# 使用 left anti join 找出从未出现在 daily_2024 的站\n",
    "missing_in_daily = (stations_df\n",
    "    .join(daily_station_ids_2024, stations_df.ID == daily_station_ids_2024.ID_IN_DAILY, how=\"left_anti\")\n",
    ")\n",
    "\n",
    "missing_count = missing_in_daily.count()\n",
    "missing_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a238f",
   "metadata": {},
   "source": [
    "# 2. Analysis（分析问答）\n",
    "\n",
    "> 对照 Assignment 1 – Analysis Q1–Q3。强调 **方法解释（Reasoning）** 与 **效率**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc874a2",
   "metadata": {},
   "source": [
    "## 2.1 Q1: 站点概况统计（总数、2025活跃、网络归属、南半球、美国属地、国家/州统计）\n",
    "\n",
    "**要点**：  \n",
    "- 2025 活跃：`LASTYEAR_ANY >= 2025` 或结合 daily 2025 是否有观测。  \n",
    "- 南半球：`LATITUDE < 0`。  \n",
    "- 网络字段：`GSN_FLAG` / `HCN_CRN`。  \n",
    "- 国家/州分布：统计并保存。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b19bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总站点数\n",
    "total_stations = stations_df.count()\n",
    "\n",
    "# 2025 活跃（基于 inventory 汇总）\n",
    "active_2025 = stations_enriched.filter(F.col(\"LASTYEAR_ANY\") >= 2025).count()\n",
    "\n",
    "# 网络归属统计（示例：是否在 GSN/HCN/CRN，具体取值需根据数据取样验证）\n",
    "network_counts = (stations_enriched\n",
    "    .select(\n",
    "        F.when(F.col(\"GSN_FLAG\").isNotNull() & (F.col(\"GSN_FLAG\") != \"\"), 1).otherwise(0).alias(\"is_GSN\"),\n",
    "        F.when(F.col(\"HCN_CRN\").contains(\"HCN\"), 1).otherwise(0).alias(\"is_HCN\"),\n",
    "        F.when(F.col(\"HCN_CRN\").contains(\"CRN\"), 1).otherwise(0).alias(\"is_CRN\"),\n",
    "    )\n",
    "    .agg(F.sum(\"is_GSN\").alias(\"GSN\"),\n",
    "         F.sum(\"is_HCN\").alias(\"HCN\"),\n",
    "         F.sum(\"is_CRN\").alias(\"CRN\"))\n",
    ")\n",
    "\n",
    "# 南半球站点数\n",
    "southern_hemisphere = stations_df.filter(F.col(\"LATITUDE\") < 0).count()\n",
    "\n",
    "# 美国属地（国家名中包含 United States 但不等于 United States）\n",
    "us_territories = stations_enriched.filter(\n",
    "    (F.col(\"COUNTRY_NAME\").contains(\"United States\")) & (F.col(\"COUNTRY_NAME\") != \"United States\")\n",
    ").count()\n",
    "\n",
    "# 按国家与州统计并保存\n",
    "by_country = (stations_enriched.groupBy(\"COUNTRY_CODE\", \"COUNTRY_NAME\").count())\n",
    "by_state = (stations_enriched.filter(F.col(\"STATE\").isNotNull())\n",
    "            .groupBy(\"STATE\",\"STATE_NAME\").count())\n",
    "\n",
    "by_country.write.mode(\"overwrite\").parquet(USER_ROOT + \"stations_by_country.parquet\")\n",
    "by_state.write.mode(\"overwrite\").parquet(USER_ROOT + \"stations_by_state.parquet\")\n",
    "\n",
    "(total_stations, active_2025, network_counts.collect(), southern_hemisphere, us_territories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a9beef",
   "metadata": {},
   "source": [
    "## 2.2 Q2: UDF 计算地理距离（Haversine）并在 NZ 站点两两配对\n",
    "\n",
    "**要点**：  \n",
    "- 仅对 **新西兰** 站点对子做 pairwise 计算（子集更高效）。  \n",
    "- 使用 **Haversine formula** 计算球面距离（单位公里）。  \n",
    "- 找出最近两站。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选取新西兰（New Zealand）站点\n",
    "nz_stations = stations_enriched.filter(F.col(\"COUNTRY_NAME\").contains(\"New Zealand\"))                                .select(\"ID\",\"NAME\",\"LATITUDE\",\"LONGITUDE\")\n",
    "\n",
    "# 产生两两组合（SELF CROSS JOIN，注意规模；如需进一步优化，可采样或基于网格索引）\n",
    "left = nz_stations.select(\n",
    "    F.col(\"ID\").alias(\"ID_A\"),\n",
    "    F.col(\"NAME\").alias(\"NAME_A\"),\n",
    "    F.col(\"LATITUDE\").alias(\"LAT_A\"),\n",
    "    F.col(\"LONGITUDE\").alias(\"LON_A\"),\n",
    ")\n",
    "right = nz_stations.select(\n",
    "    F.col(\"ID\").alias(\"ID_B\"),\n",
    "    F.col(\"NAME\").alias(\"NAME_B\"),\n",
    "    F.col(\"LATITUDE\").alias(\"LAT_B\"),\n",
    "    F.col(\"LONGITUDE\").alias(\"LON_B\"),\n",
    ")\n",
    "\n",
    "pairs = left.crossJoin(right).filter(F.col(\"ID_A\") < F.col(\"ID_B\"))\n",
    "\n",
    "# Haversine UDF\n",
    "from math import radians, sin, cos, asin, sqrt\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0088  # mean Earth radius in km\n",
    "    phi1, phi2 = radians(lat1), radians(lat2)\n",
    "    dphi = radians(lat2 - lat1)\n",
    "    dlambda = radians(lon2 - lon1)\n",
    "    a = sin(dphi/2)**2 + cos(phi1)*cos(phi2)*sin(dlambda/2)**2\n",
    "    c = 2*asin(sqrt(a))\n",
    "    return float(R*c)\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "haversine_udf = udf(haversine_km, T.DoubleType())\n",
    "\n",
    "pairs = pairs.withColumn(\"DIST_KM\", haversine_udf(\"LAT_A\",\"LON_A\",\"LAT_B\",\"LON_B\"))\n",
    "\n",
    "closest_pair = pairs.orderBy(F.col(\"DIST_KM\").asc()).limit(1)\n",
    "closest_pair.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db98a84",
   "metadata": {},
   "source": [
    "## 2.3 Q3: Core elements 统计与 TMAX 无配对 TMIN 数量\n",
    "\n",
    "**要点**：  \n",
    "- Core elements = {TMAX, TMIN, PRCP, SNOW, SNWD}。  \n",
    "- 统计各元素观测数；统计无配对（TMAX 且同站同日无 TMIN）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c03ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 仅核心要素\n",
    "core_elems = [\"TMAX\",\"TMIN\",\"PRCP\",\"SNOW\",\"SNWD\"]\n",
    "daily_core = daily_df.filter(F.col(\"ELEMENT\").isin(core_elems))\n",
    "\n",
    "# 各元素观测数\n",
    "elem_counts = daily_core.groupBy(\"ELEMENT\").count()\n",
    "elem_counts.show()\n",
    "\n",
    "# TMAX 无配对 TMIN：基于（ID, DATE）\n",
    "tmax = daily_df.filter(F.col(\"ELEMENT\")==\"TMAX\").select(F.col(\"ID\").alias(\"ID_T\"), F.col(\"DATE\").alias(\"DATE_T\"))\n",
    "tmin = daily_df.filter(F.col(\"ELEMENT\")==\"TMIN\").select(F.col(\"ID\").alias(\"ID_N\"), F.col(\"DATE\").alias(\"DATE_N\"))\n",
    "\n",
    "tmax_no_tmin = (tmax.join(tmin, (tmax.ID_T==tmin.ID_N) & (tmax.DATE_T==tmin.DATE_N), how=\"left_anti\"))\n",
    "missing_pairs_count = tmax_no_tmin.count()\n",
    "\n",
    "# 参与这些观测的唯一站点数\n",
    "unique_stations_missing = tmax_no_tmin.select(\"ID_T\").distinct().count()\n",
    "\n",
    "(missing_pairs_count, unique_stations_missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938db02",
   "metadata": {},
   "source": [
    "# 3. Visualizations（可视化）\n",
    "\n",
    "> 对照 Assignment 1 – Visualization Q1–Q2。先在 Spark 侧聚合，再 `.toPandas()` 绘图。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7110d771",
   "metadata": {},
   "source": [
    "## 3.1 Q1: New Zealand — TMIN/TMAX 时间序列（站点子图 + 全国平均）\n",
    "\n",
    "**流程**：  \n",
    "1) 过滤 NZ 站点 ID → 过滤 daily 中 TMIN/TMAX → 选择合适的时间聚合（如月平均）。  \n",
    "2) 站点级别：每站一幅子图（subplot）。  \n",
    "3) 全国：合并后作一幅大图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3889fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 NZ 站点 ID 集合\n",
    "nz_ids_df = nz_stations.select(\"ID\").distinct()\n",
    "daily_nz_tm = (daily_df\n",
    "    .join(nz_ids_df, on=\"ID\", how=\"inner\")\n",
    "    .filter(F.col(\"ELEMENT\").isin(\"TMIN\",\"TMAX\"))\n",
    ")\n",
    "\n",
    "# 选择月平均作为平滑级别\n",
    "daily_nz_tm = daily_nz_tm.withColumn(\"YEAR\", F.year(\"DATE\")).withColumn(\"MONTH\", F.month(\"DATE\"))\n",
    "monthly_nz = (daily_nz_tm\n",
    "    .groupBy(\"ID\",\"ELEMENT\",\"YEAR\",\"MONTH\")\n",
    "    .agg(F.avg(\"VALUE\").alias(\"AVG_VALUE\"))\n",
    ")\n",
    "\n",
    "# 收集为 Pandas 后绘图（注意规模控制）\n",
    "pdf = monthly_nz.toPandas()\n",
    "\n",
    "# TODO: 使用 matplotlib/plotly 生成 subplot（每站一子图）与全国平均曲线。\n",
    "# 注意：VALUE 单位可能需换算/标注；缺口（缺失月份）需显式显示。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edda962",
   "metadata": {},
   "source": [
    "## 3.2 Q2: 全球年度降水（PRCP）— 国家级 Choropleth（2024）\n",
    "\n",
    "**流程**：  \n",
    "1) 从 daily 过滤 PRCP → 按国家+年份聚合 → 计算年度平均日降水。  \n",
    "2) 保存统计表到 `USER_ROOT`。  \n",
    "3) 与 `geopandas` 或 `plotly` 的国家名称匹配，绘制 2024 年 choropleth。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算年度国家平均日降水（示例：2024 年）\n",
    "year_target = 2024\n",
    "daily_prcp = daily_df.filter(F.col(\"ELEMENT\")==\"PRCP\").withColumn(\"YEAR\", F.year(\"DATE\"))\n",
    "\n",
    "# 将站点与国家 join（使用 enriched_stations）\n",
    "daily_prcp_country = (daily_prcp\n",
    "    .join(stations_enriched.select(\"ID\",\"COUNTRY_CODE\",\"COUNTRY_NAME\"), on=\"ID\", how=\"left\")\n",
    "    .filter(F.col(\"YEAR\")==year_target)\n",
    ")\n",
    "\n",
    "prcp_by_country_year = (daily_prcp_country\n",
    "    .groupBy(\"COUNTRY_CODE\",\"COUNTRY_NAME\",\"YEAR\")\n",
    "    .agg(F.avg(\"VALUE\").alias(\"AVG_DAILY_PRCP\"))\n",
    ")\n",
    "\n",
    "# 保存结果\n",
    "(prcp_by_country_year\n",
    " .write.mode(\"overwrite\")\n",
    " .parquet(USER_ROOT + f\"prcp_by_country_year_{year_target}.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO（可视化）：\n",
    "# 1) prcp_by_country_year_{year}.parquet 读取为 Pandas\n",
    "# 2) 与自然地理国家边界（geopandas 自带或 naturalearth_lowres）匹配\n",
    "# 3) 绘制 choropleth（注意：投影选择、配色、缺失国家处理、异常值注记）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c62706f",
   "metadata": {},
   "source": [
    "# 4. 报告写作（Writing）与导出（Export）\n",
    "\n",
    "- 在每个问题完成后，记录：**Methodology（方法）** → **Results（结果）** → **Reasoning（解释）**。  \n",
    "- 将表格和图形导出到 supplementary material（zip）。  \n",
    "- 在报告正文中引用图表（Figure X / Table Y）并说明含义。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f884407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可选：将关键统计导出为 CSV 便于在报告中引用\n",
    "# 示例：elem_counts.toPandas().to_csv('/mnt/data/core_element_counts.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d2b9ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 结束（Finish）\n",
    "\n",
    "- 请确保：所有路径正确、输出已写入 `USER_ROOT`、Notebook 无报错。  \n",
    "- 提交：报告 PDF（3,000–5,000 字）+ supplementary zip（代码、图、脚本；不包含数据输出）。\n",
    "\n",
    "> Good luck! 🚀\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
